# Mirrors `config/example_config_reppo.yaml` but scoped to `runner.*` for Hydra.
class_name: OnPolicyRunner

# General
num_steps_per_env: 64
max_iterations: 5000
seed: 1

# Observations
obs_groups: {policy: [policy], critic: [critic]}

# Logging
save_interval: 50
experiment_name: walking_experiment
run_name: ""
logger: wandb  # tensorboard, neptune, wandb
neptune_project: legged_gym
wandb_project: legged_gym

policy:
  class_name: ActorQ
  activation: swish
  actor_obs_normalization: true
  critic_obs_normalization: true
  actor_hidden_dims: [512, 512, 256]
  critic_hidden_dims: [1024, 512, 256]
  num_critic_bins: 151
  vmin: -20.0
  vmax: 20.0
  init_noise_std: 0.1
  noise_std_type: sigmoid
  state_dependent_std: true
  distribution_type: tanh  # normal or tanh
  init_alpha_temp: 0.01
  init_alpha_kl: 0.1

algorithm:
  class_name: REPPO
  learning_rate: 0.0003
  num_learning_epochs: 8
  num_mini_batches: 32
  desired_kl: 0.1
  target_entropy: -0.5
  gamma: 0.99
  lam: 0.95
  max_grad_norm: 1.0

  scale_actions: false
  action_upper_bound: 1.0
  action_lower_bound: -1.0

  rnd_cfg: null
  symmetry_cfg: null
